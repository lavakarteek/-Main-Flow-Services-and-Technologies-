{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavakarteek/-Main-Flow-Services-and-Technologies-/blob/main/Akridata_ML_Intern_Sep25_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook contains the coding assignment for Akridata's ML internships (September 2025).\n",
        "\n",
        "  * [Job description](https://docs.google.com/document/d/e/2PACX-1vRwHkY0TLebdoe-meFsgM3p3DdReJq5Kl1fVN5-tHZrYEuXwo7vYvbIkxa2oQNH4D26Hsv9wP7Mup1l/pub)\n",
        "  * [Application form](https://docs.google.com/forms/d/e/1FAIpQLSdTW9OQXXlHkjKCcxgil4H3BgwPvOrlnqzL0JsqBh5xyAK0aQ/viewform?usp=dialog)\n",
        "\n",
        "## Why This Assignment Exists\n",
        "\n",
        "We receive far more applications than we can meaningfully review — it's too easy to click “Apply” online. That flood inevitably includes many who aren't a fit for the role. We don't want to filter by college names or grades alone.\n",
        "\n",
        "So we add **a bit of friction**: a real, hands-on task. Yes, it's extra work — but it's a better signal than a resume or a keyword-stuffed LinkedIn profile.\n",
        "\n",
        "We've picked something that:\n",
        "- Is technically straightforward to complete in a reasonable time.\n",
        "- Is interesting and directly relevant to the problems we solve.\n",
        "- Gives you a taste of how we actually work.\n",
        "\n",
        "You are free to use any tools (Google, Forums, Cursor, GenAI) you want to complete this assignment - as long as you do it yourself. You must understand what you do though. We may quiz you on this if you get shortlisted.\n",
        "\n",
        "## Publishing Your Code\n",
        "\n",
        "Once the interview process wraps up, we encourage you to publish your solution and add it to your portfolio. Until then, please keep it private so everyone gets a fair shot.\n",
        "\n",
        "## The Task: Defect Classification\n",
        "\n",
        "At Akridata, we focus on the **critical edge cases of manufacturing inspection** — where failures are costly, dangerous, or both.  \n",
        "Think precision lenses and glass components that could shatter, ball bearings that can fail in high-stress machinery, or kilometers of railway track that no human can realistically inspect end-to-end. We also handle inspection problems where rules-based computer vision falls short — like detecting subtle miswirings in cable assemblies, spotting PCB defects in complex layouts, or checking extremely small parts where manual inspection is painfully slow.\n",
        "\n",
        "This task gives you a glimpse into vision-based inspection work.  \n",
        "You'll tackle **zero-shot defect classification** for a real-world-like case: detecting surface defects in hazelnut images from the MVTec-AD dataset, using OpenAI's `CLIP` vision-language model.\n"
      ],
      "metadata": {
        "id": "1XsPE95HC8AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT\n",
        "\n",
        "## Before proceeding, copy this notebook into your Google Drive, and make changes to your local copy.\n",
        "\n",
        "You have to share the link to your local copy."
      ],
      "metadata": {
        "id": "waDa7t2OG8M9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Defect Classification with CLIP (Git + Colab Workflow)\n",
        "\n",
        "Everything you have to do is templated in the notebook. We provide an overview here; but go through the rest of the notebook before you start coding.\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this assignment, you will:\n",
        "- Use OpenAI's CLIP model to perform zero-shot defect classification on the `hazelnut` subset of the MVTec-AD dataset (test split).\n",
        "- Implement a configuration class (Pydantic) and a classification function in Python.\n",
        "- Practice basic Git workflows: branching, committing, merging, and preserving commit history.\n",
        "- Run your completed code on real image data and produce a confusion matrix.\n",
        "\n",
        "All work must be done entirely inside this Colab notebook, including:\n",
        "- Running Git commands\n",
        "- Editing files in the cloned repository\n",
        "- Installing and importing packages\n",
        "\n",
        "We will run the code in this notebook to verify the submission.\n",
        "\n",
        "*The cells marked as 'TO DO' are the ones where you have to edit code (or text).*\n",
        "\n",
        "\n",
        "## References\n",
        "- [CLIP](https://github.com/openai/CLIP)\n",
        "- [MVTec-AD](https://www.mvtec.com/company/research/datasets/mvtec-ad)\n",
        "- [Pydantic](https://docs.pydantic.dev/latest/)\n",
        "- [CLIP-AC (WinCLIP)](https://arxiv.org/abs/2303.14814)\n",
        "  - See section 4.1 in the WinCLIP paper for a\n",
        "    basic two-class design for anomaly classification.\n",
        "  - Extend this to a defect classification problem for multiple defect classes for this task.\n",
        "  - **Note:** Section 4.1 in the WinCLIP paper does *anomaly classification* only; i.e., separate normal images from defective images. It **does not** discriminate the different types of defects. Your task is to classify good as well as different defect types. For example, the class names for some problem will be ['good', 'broken', 'bent', 'cut', etc.]. These are **not** the class names for the current problem - just as an example for the defect classification task."
      ],
      "metadata": {
        "id": "Uni69hgrOgEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and extract dataset"
      ],
      "metadata": {
        "id": "vpn8zXSBuREa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download a the dataset sample from Supervisely\n",
        "!curl https://assets.supervisely.com/supervisely-supervisely-assets-public/teams_storage/W/6/pt/ANAGVgKaC62tTrDQWK5JhNP2dd8ynqaTKSM1QdVoAasmTdaLvBwCuW7nCrq9o9lLS2padKnV9QogVGFlEPg7vxEBPIfuFC2Yq7ELNW7xn2t1egLrQPoGpNFJobhh.tar --output mvtec.tar"
      ],
      "metadata": {
        "id": "re_O7sYqsyMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract the tarball 'mvtec.tar' and look for test images for the hazelnut subset\n",
        "!tar -xf mvtec.tar\n",
        "!ls test/img/hazelnut_*"
      ],
      "metadata": {
        "id": "MZA-1oGGtLlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the repo\n",
        "\n",
        "This is the repo to clone:\n",
        "\n",
        "https://github.com/akridata-ai/ZS-CLIP-AC-naive"
      ],
      "metadata": {
        "id": "OTxq9DihueUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Clone the Git repo here, in the Colab runtime"
      ],
      "metadata": {
        "id": "czOSoKfOuhx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure git user\n",
        "!git config --global user.email \"test-user@email.com\"\n",
        "!git config --global user.name \"Test User\""
      ],
      "metadata": {
        "id": "zLGzAuCBwyry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Move to the repo for subsequent work\n",
        "%cd /content/ZS-CLIP-AC-naive"
      ],
      "metadata": {
        "id": "sYD_wdCBut3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Checkout the EXISTING feature branch in the repository. What's it called? How do you find the names of existing branches?\n",
        "# DO NOT create a new branch!\n"
      ],
      "metadata": {
        "id": "l4KFkPXUvu8U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify that the files appear here\n",
        "!ls"
      ],
      "metadata": {
        "id": "8KNo7_WGyQB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Move images for the hazelnut subset from the test split to the `data` folder in the repository\n",
        "!cp /content/test/img/hazelnut_* data/"
      ],
      "metadata": {
        "id": "E4G1dClq4Srj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install the requirements from the `requirements.txt` file\n",
        "!pip install -qr requirements.txt"
      ],
      "metadata": {
        "id": "OhyMN6QOyyzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "Go through the [CLIP github repo](https://github.com/openai/CLIP) to understand zero-shot classification. Go through section 4.1 in the [WinCLIP paper](https://arxiv.org/abs/2303.14814) to understand zero-shot anomaly classification; they call this `CLIP-AC`.\n",
        "\n",
        "Your task is to extend anomaly classification (just normal vs defective) to do defect classification (normal vs different categories of defects) using the CLIP model.\n",
        "\n",
        "Your model should predict non-defective images as \"good\". For defective images, the defect category should be predicted as the class name. See the image filenames to know the defect categories; or see the [MVTec-AD dataset details](https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf). This task is only for the `hazelnut` subset.\n",
        "\n",
        "Note that the 'clip' package from OpenAI is installed as part of the requirements in the previous cell."
      ],
      "metadata": {
        "id": "HuBG3dDi8FAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Modify `spec.py` and save its contents\n",
        "%%writefile spec.py\n",
        "\"\"\"\n",
        "Spec containing the configuration for the defect classification task\n",
        "\"\"\"\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class DefectClassificationSpec(BaseModel):\n",
        "    \"\"\"\n",
        "    Configuration for defect classification.\n",
        "\n",
        "    Attributes:\n",
        "        class_names: List[str]; The list of class names (normal, defect categories).\n",
        "        prompts: List[str]; The list of prompts corresponding to the class names.\n",
        "        model_name (str): Name of the CLIP model variant to use.\n",
        "    \"\"\"\n",
        "    # TODO: Define attributes here with appropriate types and default values if any.\n",
        "    pass"
      ],
      "metadata": {
        "id": "n42_SESSvxgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Modify `clip_ac.py` and save its contents\n",
        "%%writefile clip_ac.py\n",
        "\"\"\"\n",
        "Zero-shot defect classification using CLIP\n",
        "\"\"\"\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from spec import DefectClassificationSpec\n",
        "\n",
        "def classify_defects(spec: DefectClassificationSpec, test_dir: Path) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Zero-shot defect classification using CLIP.\n",
        "\n",
        "    Args:\n",
        "        spec: Pydantic spec containing prompts and model name.\n",
        "        test_dir: Path to the test dataset root.\n",
        "                For this task, we expect filenames to indicate the ground-truth label.\n",
        "                E.g. `hazelnut_print_*.png` indicates an image with the 'print' defect class.\n",
        "                There are a few other kinds of defects too; find these from the file names.\n",
        "                `hazelnut_good_*.png` indicates an image of the good/normal class.\n",
        "\n",
        "    Returns:\n",
        "        y_true: list of ground-truth labels\n",
        "        y_pred: list of predicted labels\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(spec.model_name, device=device)\n",
        "\n",
        "    # TODO: Load images and labels from test_dir\n",
        "    # TODO: Encode text prompts for all the classes\n",
        "    # TODO: Compare image features to text features, choose predicted label\n",
        "    # TODO: Return lists y_true and y_pred\n",
        "    raise NotImplementedError(\"Classification logic not implemented yet.\")"
      ],
      "metadata": {
        "id": "vodMkfqP8AWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Run the defect classification code\n",
        "from pathlib import Path\n",
        "from spec import DefectClassificationSpec\n",
        "from clip_ac import classify_defects\n",
        "\n",
        "# Create the spec for the 'hazelnut' subset\n",
        "defect_spec = DefectClassificationSpec(...)  # Pass the right class names and prompts\n",
        "\n",
        "test_dir = Path(\"data\")\n",
        "y_true, y_pred = classify_defects(defect_spec, test_dir)"
      ],
      "metadata": {
        "id": "KFk0HjcZ8ZrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Print the confusion matrix using the predictions and ground truth labels"
      ],
      "metadata": {
        "id": "06ea5MK29WdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations (TO DO)\n",
        "\n",
        "What did you observe? Will a different classification spec improve things? If yes, then try it out below."
      ],
      "metadata": {
        "id": "Ue3yl27g98Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional/Bonus] Rerun 1: With a different spec"
      ],
      "metadata": {
        "id": "gQwmPBQm-LBc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elDl6oix-MAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bring code into the main branch"
      ],
      "metadata": {
        "id": "HUvORgQE8eq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Commit the changes to the python files to the current feature branch\n"
      ],
      "metadata": {
        "id": "xt6lJ9nI8iW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Switch to the main branch, and merge the feature branch into the main branch\n"
      ],
      "metadata": {
        "id": "trcQs5Yn8on2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Verify git log to see the latest commit\n"
      ],
      "metadata": {
        "id": "ObNPqILE8t3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TO DO: Display the changes in the `spec.py` file in the main branch"
      ],
      "metadata": {
        "id": "9sbVemf88yV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "\n",
        "* Verify that this Colab notebook is owned by you, and in your Google Drive.\n",
        "* Change the 'Share' settings to 'Anyone with the link' - give read access only.\n",
        "* Copy the link to the notebook through the 'Share' settings.\n",
        "* Open a new anonymous browser window, where you are not signed in. Verify that the link you copied opens the notebook, and that all outputs are visible.\n",
        "* Submit this link through the internship application form."
      ],
      "metadata": {
        "id": "b8Deeu1yHHwc"
      }
    }
  ]
}